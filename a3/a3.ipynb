{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Answers\n",
    "\n",
    "In this assignment, you will build a neural dependency parser using PyTorch. In Part 1, you will learn\n",
    "about two general neural network techniques (Adam Optimization and Dropout) that you will use to build\n",
    "the dependency parser in Part 2. In Part 2, you will implement and train the dependency parser, before\n",
    "analyzing a few erroneous dependency parses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine Learning & Neural Networks (8 points)\n",
    "\n",
    "(a) (4 points) Adam Optimizer\n",
    "Recall the standard Stochastic Gradient Descent update rule:\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta J_{minibatch}(\\theta)$$\n",
    "\n",
    "where $\\theta$ is a vector containing all of the model parameters, $J$ is the loss function, $\\nabla \\theta J_{minibatch}(\\theta)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\\alpha$ is the learning rate. \n",
    "\n",
    "[Adam Optimization](https://arxiv.org/pdf/1412.6980.pdf) uses a more sophisticated update rule with two additional steps:\n",
    "\n",
    "**i.(2 points)** First, *Adam* uses a trick called momentum by keeping track of m, a rolling average\n",
    "of the gradients:\n",
    "\\begin{align}\n",
    "m &\\leftarrow \\beta_1 m + (1-\\beta_1)  \\nabla_\\theta J_{minibatch}(\\theta)\\\\\n",
    "\\theta &\\leftarrow \\theta - \\alpha m\n",
    "\\end{align}\n",
    "where $β_1$ is a hyperparameter between 0 and 1 (often set to 0.9). Briefly explain (you don’t need\n",
    "to prove mathematically, just give an intuition) how using m stops the updates from varying\n",
    "as much and why this low variance may be helpful to learning, overall.\n",
    "\n",
    "<font color='red'>Answer</font>: The momentum tracks a \"history\" of the last movement in the hyperparameter space. Then, by using a weighted average by* $\\beta_1$, we just use an \"amount\" of the new change produced by the gradient, and not totally changing the overall direction in one step.\n",
    "\n",
    "This technique avoids \"zigzags\" in the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii. (2 points)** Adam also uses adaptive learning rates by keeping track of v, a rolling average of\n",
    "the magnitudes of the gradients:\n",
    "\\begin{align}\n",
    "m &\\leftarrow \\beta_1 m + (1-\\beta_1)  \\nabla_\\theta J_{minibatch}(\\theta)\\\\\n",
    "v &\\leftarrow \\beta_2 v + (1-\\beta_2)  \\nabla_\\theta J_{minibatch}(\\theta) \\odot \\nabla_\\theta J_{minibatch}(\\theta)\\\\\n",
    "θ &← θ − α \\odot \\frac{m}{\\sqrt{v}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\odot$ and / denote elementwise multiplication and division (so $z \\odot z$ is elementwise squaring)\n",
    "and $\\beta_2$ is a hyperparameter between 0 and 1 (often set to 0.99). Since Adam divides the update\n",
    "by $\\sqrt{v}$, which of the model parameters will get larger updates? Why might this help with\n",
    "learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='red'>Answer</font>: \n",
    "We can, in an abuse of notation, say that: \n",
    "$$\n",
    "\\nabla_\\theta J_{minibatch}(\\theta) \\odot \\nabla_\\theta J_{minibatch}(\\theta) = \\nabla_\\theta J_{minibatch}(\\theta)^2\n",
    "$$\n",
    "\n",
    "That is, each component of the gradient is squared. \n",
    "\n",
    "So, the recurrent equation of $v$ is just a moving average of the squares of each component. We can see it as a second moment estimation of the gradient, that is, if we think that the gradient is a random vector $X$ we are computing the uncentered variance of each component ($E[X^2]$). Recall that the variance of a random variable is $Var[X] = E[X^2] - E[X]^2$.\n",
    "\n",
    "Dividing by the square root of the variance (that is, the deviation) will normalize the values of $v$. Thus, a value with a tiny derivative will have the same step as one with a big derivative, smoothing the step across the parameters. (Am I sure of this?))\n",
    "\n",
    "In proportion, values with a small variance will likely receive a bigger update, helping to get them off of possibly saddle points or local minima. Also, values will large variances will receive proportionally smaller updates to prevent overshooting in a wrong direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)(4 points)** Dropout 3 is a regularization technique. During training, dropout randomly sets units\n",
    "in the hidden layer $h$ to zero with probability $p_{drop}$ (dropping different units each minibatch), and\n",
    "then multiplies h by a constant $\\gamma$. We can write this as\n",
    "\n",
    "$$\n",
    "h_{drop} = \\gamma d \\odot h\n",
    "$$\n",
    "\n",
    "**i. (2 points)** What must γ equal in terms of p drop ? Briefly justify your answer.\n",
    "\n",
    "<font color='red'>Answer</font>: \n",
    "We want $E[h_{drop}]_i = h_i$, \n",
    "\n",
    "$$\n",
    "h_{drop, i} =  \\gamma d_i h_i$$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "E[h_{drop, i}] =  \\gamma (1 - p_{drop}) h_i\n",
    "$$\n",
    "\n",
    "To make this product equal to $h_i$, we need $$\\gamma = \\frac{1}{1-p_{drop}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**ii. (2 points)** Why should we apply dropout during training but not during evaluation?\n",
    "\n",
    "Because we want to use all the neurons we have trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Transition-based dependency parsing\n",
    "\n",
    "We’d like to look at example dependency parses and understand where parsers like ours\n",
    "might be wrong. For example, in this sentence:\n",
    "\n",
    "<img src=\"imgs/moscow_error.png\"/>\n",
    "\n",
    "the dependency of the phrase *into Afghanistan* is wrong, because the phrase should modify sent (as\n",
    "in sent into Afghanistan) not troops (because troops into Afghanistan doesn’t make sense). Here is\n",
    "the correct parse:\n",
    "\n",
    "<img src=\"imgs/moscow_ok.png\"/>\n",
    "\n",
    "\n",
    "There are four types of parsing error:\n",
    "- **Prepositional Phrase Attachment Error**: In the example above, the phrase into Afghanistan\n",
    "is a prepositional phrase. A Prepositional Phrase Attachment Error is when a prepositional\n",
    "phrase is attached to the wrong head word (in this example, troops is the wrong head word and\n",
    "sent is the correct head word). More examples of prepositional phrases include with a rock,\n",
    "before midnight and under the carpet.\n",
    "- **Verb Phrase Attachment Error**: In the sentence *Leaving the store unattended, I went\n",
    "outside to watch the parade*, the phrase *leaving the store* unattended is a verb phrase. A Verb\n",
    "Phrase Attachment Error is when a verb phrase is attached to the wrong head word (in this\n",
    "example, the correct head word is went).\n",
    "- **Modifier Attachment Error**: In the sentence I am extremely short, the adverb extremely is\n",
    "a modifier of the adjective short. A Modifier Attachment Error is when a modifier is attached\n",
    "to the wrong head word (in this example, the correct head word is short).\n",
    "- **Coordination Attachment Error**: In the sentence Would you like brown rice or garlic naan?,\n",
    "the phrases brown rice and garlic naan are both conjuncts and the word or is the coordinating\n",
    "conjunction. The second conjunct (here garlic naan) should be attached to the first conjunct\n",
    "(here brown rice). A Coordination Attachment Error is when the second conjunct is attached\n",
    "to the wrong head word (in this example, the correct head word is rice). Other coordinating\n",
    "conjunctions include and, but and so.\n",
    "\n",
    "\n",
    "In this question are four sentences with dependency parses obtained from a parser. Each sentence\n",
    "has one error, and there is one example of each of the four types above. For each sentence, state\n",
    "the type of error, the incorrect dependency, and the correct dependency. To demonstrate: for the\n",
    "example above, you would write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/error_1.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='red'>Answer</font>: There is a verb phrase (*fearing my death*) incorrectly attached to *wedding*. So the error here is one of the **Verb Phrase Attachment Error**\n",
    "\n",
    "To correct it, we should remove the mentioned dependency. **fearing** should be dependent of **heading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/error_2.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Answer</font>: Uhmmm.... I suppose this is a coordination error, but I haven't seen coordination errors so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/error_3.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Answer</font>: \n",
    "\n",
    "Error: **Prepositional Phrase Attachment Error**. \"In Midland, Texas\" is a prepositional phrase attached to the subject of the sentence (\"It is on loan in Midland, Texas\"). \n",
    "\n",
    "So, remove named--> Midland, and make Midland dependent of *loan*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/error_4.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Answer</font>: \n",
    "\n",
    "Error: ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
