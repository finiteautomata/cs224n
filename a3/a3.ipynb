{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Answers\n",
    "\n",
    "In this assignment, you will build a neural dependency parser using PyTorch. In Part 1, you will learn\n",
    "about two general neural network techniques (Adam Optimization and Dropout) that you will use to build\n",
    "the dependency parser in Part 2. In Part 2, you will implement and train the dependency parser, before\n",
    "analyzing a few erroneous dependency parses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine Learning & Neural Networks (8 points)\n",
    "\n",
    "(a) (4 points) Adam Optimizer\n",
    "Recall the standard Stochastic Gradient Descent update rule:\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta J_{minibatch}(\\theta)$$\n",
    "\n",
    "where $\\theta$ is a vector containing all of the model parameters, $J$ is the loss function, $\\nabla \\theta J_{minibatch}(\\theta)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\\alpha$ is the learning rate. \n",
    "\n",
    "[Adam Optimization](https://arxiv.org/pdf/1412.6980.pdf) uses a more sophisticated update rule with two additional steps:\n",
    "\n",
    "**i.(2 points)** First, *Adam* uses a trick called momentum by keeping track of m, a rolling average\n",
    "of the gradients:\n",
    "\\begin{align}\n",
    "m &\\leftarrow \\beta_1 m + (1-\\beta_1)  \\nabla_\\theta J_{minibatch}(\\theta)\\\\\n",
    "\\theta &\\leftarrow \\theta - \\alpha m\n",
    "\\end{align}\n",
    "where $β_1$ is a hyperparameter between 0 and 1 (often set to 0.9). Briefly explain (you don’t need\n",
    "to prove mathematically, just give an intuition) how using m stops the updates from varying\n",
    "as much and why this low variance may be helpful to learning, overall.\n",
    "\n",
    "<font color='red'>Answer</font>: The momentum tracks a \"history\" of the last movement in the hyperparameter space. Then, by using a weighted average by* $\\beta_1$, we just use an \"amount\" of the new change produced by the gradient, and not totally changing the overall direction in one step.\n",
    "\n",
    "This technique avoids \"zigzags\" in the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii. (2 points)** Adam also uses adaptive learning rates by keeping track of v, a rolling average of\n",
    "the magnitudes of the gradients:\n",
    "\\begin{align}\n",
    "m &\\leftarrow \\beta_1 m + (1-\\beta_1)  \\nabla_\\theta J_{minibatch}(\\theta)\\\\\n",
    "v &\\leftarrow \\beta_2 v + (1-\\beta_2)  \\nabla_\\theta J_{minibatch}(\\theta) \\odot \\nabla_\\theta J_{minibatch}(\\theta)\\\\\n",
    "θ &← θ − α \\odot \\frac{m}{\\sqrt{v}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\odot$ and / denote elementwise multiplication and division (so $z \\odot z$ is elementwise squaring)\n",
    "and $\\beta_2$ is a hyperparameter between 0 and 1 (often set to 0.99). Since Adam divides the update\n",
    "by $\\sqrt{v}$, which of the model parameters will get larger updates? Why might this help with\n",
    "learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='red'>Answer</font>: \n",
    "We can, in an abuse of notation, say that: \n",
    "$$\n",
    "\\nabla_\\theta J_{minibatch}(\\theta) \\odot \\nabla_\\theta J_{minibatch}(\\theta) = \\nabla_\\theta J_{minibatch}(\\theta)^2\n",
    "$$\n",
    "\n",
    "That is, each component of the gradient is squared. \n",
    "\n",
    "So, the recurrent equation of $v$ is just a moving average of the squares of each component. We can see it as a second moment estimation of the gradient, that is, if we think that the gradient is a random vector $X$ we are computing the uncentered variance of each component ($E[X^2]$). Recall that the variance of a random variable is $Var[X] = E[X^2] - E[X]^2$.\n",
    "\n",
    "Dividing by the square root of the variance (that is, the deviation) will normalize the values of $v$. Thus, a value with a tiny derivative will have the same step as one with a big derivative, smoothing the step across the parameters. (Am I sure of this?))\n",
    "\n",
    "In proportion, values with a small variance will likely receive a bigger update, helping to get them off of possibly saddle points or local minima. Also, values will large variances will receive proportionally smaller updates to prevent overshooting in a wrong direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
