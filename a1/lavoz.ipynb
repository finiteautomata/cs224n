{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224N Assignment 1: Exploring Word Vectors (25 Points)\n",
    "\n",
    "Welcome to CS224n! \n",
    "\n",
    "Before you start, make sure you read the README.txt in the same directory as this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# All the dependencies you need, can be installed by running .\n",
    "# ----------------\n",
    "\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lavoz.corpus\") as f:\n",
    "    news = list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what these documents are like…."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['la', 'ex', 'ministra', 'de', 'salud', 'graciela', 'ocaña', 'solicitará',\n",
      " 'esta', 'semana', 'a', 'la', 'justicia', 'federal', 'desarchivar', 'una',\n",
      " 'denuncia', 'por', 'supuestas', 'facturas', 'apócrifas', 'que', 'tendrían',\n",
      " 'relación', 'colateral', 'con', 'la', 'causa', 'de', 'la', 'mafia', 'de',\n",
      " 'los', 'medicamentos', 'la', 'presentación', 'de', 'ocaña', 'en', 'los',\n",
      " 'tribunales', 'porteños', 'podría', 'salpicar', 'a', 'empresas', 'del', 'ramo',\n",
      " 'en', 'córdoba', 'el', 'pedido', 'se', 'sustentará', 'en', 'la', 'presunción',\n",
      " 'de', 'que', 'puede', 'haber', 'elementos', 'que', 'permitan', 'avanzar', 'en',\n",
      " 'la', 'investigación', 'en', 'las', 'nuevas', 'escuchas', 'telefónicas', 'que',\n",
      " 'agregó', 'el', 'juez', 'federal', 'norberto', 'oyarbide', 'a', 'la', 'causa',\n",
      " 'que', 'tiene', 'a', 'su', 'cargo', 'en', 'noviembre', 'pasado', 'ocaña',\n",
      " 'ministra', 'de', 'salud', 'del', 'gobierno', 'de', 'cristina', 'fernández',\n",
      " 'entre', 'diciembre', 'de', 'y', 'junio', 'de', 'pidió', 'al', 'fiscal',\n",
      " 'porteño', 'jorge', 'di', 'lello', 'una', 'investigación', 'preliminar',\n",
      " 'sobre', 'presuntas', 'facturas', 'apócrifas', 'que', 'recayó', 'en', 'el',\n",
      " 'juez', 'federal', 'sergio', 'torres', 'y', 'que', 'al', 'mes', 'siguiente',\n",
      " 'archivó', 'su', 'colega', 'octavio', 'aráoz', 'de', 'lamadrid', 'pero',\n",
      " 'ahora', 'ocaña', 'considera', 'que', 'en', 'las', 'nuevas', 'escuchas',\n",
      " 'sumadas', 'por', 'oyarbide', 'que', 'trascendieron', 'periodísticamente',\n",
      " 'puede', 'haber', 'elementos', 'para', 'profundizar', 'la', 'investigación',\n",
      " 'según', 'dijo', 'a', 'este', 'diario', 'la', 'propia', 'ex', 'ministra',\n",
      " 'por', 'eso', 'le', 'pedirá', 'también', 'a', 'torres', 'que', 'le',\n",
      " 'solicite', 'a', 'oyarbide', 'el', 'acceso', 'a', 'esas', 'escuchas', 'la',\n",
      " 'denuncia', 'en', 'cuestión', 'tiene', 'que', 'ver', 'con', 'seis', 'facturas',\n",
      " 'que', 'entre']\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize\n",
    "from pprint import pprint\n",
    "\n",
    "#print(news[0])\n",
    "def tokenize(text):\n",
    "\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language='spanish')\n",
    "    tokens = [t.lower() for t in tokens if t[0].isalpha()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "pprint(tokenize(news[0])[:200], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_news = [tokenize(n) for n in news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "counter = {}\n",
    "\n",
    "for doc in tokenized_news:\n",
    "    for tok in doc:\n",
    "        counter[tok] = counter.get(tok, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.Series(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántas quedan con más de 3 ocurrencias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(words, range=(0, 2000), bins=50)\n",
    "\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25414"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(words > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nos quedamos con  25414\n"
     ]
    }
   ],
   "source": [
    "relevant_words = words[words > 3].index\n",
    "word2Ind = {word:k for k, word in enumerate(relevant_words)}\n",
    "\n",
    "num_words = len(relevant_words)\n",
    "\n",
    "print(\"Nos quedamos con \", num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 s, sys: 276 ms, total: 23.8 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "window_size = 5\n",
    "# ------------------\n",
    "# Write your implementation here.\n",
    "\n",
    "co_occ = {}\n",
    "\n",
    "for doc in tokenized_news:\n",
    "    for center_idx, center_word in enumerate(doc):\n",
    "        try:\n",
    "            i = word2Ind[center_word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for context_word in doc[max(center_idx-window_size, 0):center_idx+window_size+1]:\n",
    "            try:\n",
    "                j = word2Ind[context_word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            if i != j:\n",
    "                co_occ[(i, j)] = co_occ.get((i, j), 0)+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "M = dok_matrix((num_words, num_words))\n",
    "\n",
    "for coords, value in co_occ.items():\n",
    "    M[coords] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = M.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Truncated SVD over 25414 words...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "    to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "        - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "    Params:\n",
    "        M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n",
    "        k (int): embedding size of each word after dimension reduction\n",
    "    Return:\n",
    "        M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                In terms of the SVD from math class, this actually returns U * S\n",
    "\"\"\"    \n",
    "n_iters = 30     # Use this parameter in your call to `TruncatedSVD`\n",
    "\n",
    "svd = TruncatedSVD(n_components=60, n_iter=n_iters)\n",
    "print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "\n",
    "svd.fit(M)\n",
    "\n",
    "M_reduced = svd.transform(M)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_norm = M_reduced / np.linalg.norm(M_reduced, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Palabras más cercanas a macri\n",
      "\n",
      " 1. salió\n",
      " 2. chávez\n",
      " 3. irán\n",
      " 4. mauricio\n",
      " 5. sánchez\n",
      " 6. quien\n",
      " 7. rousseff\n",
      " 8. cargo\n",
      " 9. mubarak\n",
      "10. china\n",
      "================================================================================\n",
      "Palabras más cercanas a cristina\n",
      "\n",
      " 1. fernández\n",
      " 2. presidenta\n",
      " 3. nadia\n",
      " 4. kirchner\n",
      " 5. michelle\n",
      " 6. elisa\n",
      " 7. electa\n",
      " 8. moyano\n",
      " 9. néstor\n",
      "10. fortuna\n",
      "================================================================================\n",
      "Palabras más cercanas a izquierda\n",
      "\n",
      " 1. educación\n",
      " 2. salud\n",
      " 3. producción\n",
      " 4. seguridad\n",
      " 5. gravedad\n",
      " 6. venta\n",
      " 7. compra\n",
      " 8. economía\n",
      " 9. recolección\n",
      "10. voluntad\n",
      "================================================================================\n",
      "Palabras más cercanas a derecha\n",
      "\n",
      " 1. mundial\n",
      " 2. industria\n",
      " 3. propiedad\n",
      " 4. juventud\n",
      " 5. revolución\n",
      " 6. cultura\n",
      " 7. social\n",
      " 8. economía\n",
      " 9. estabilidad\n",
      "10. unidad\n",
      "================================================================================\n",
      "Palabras más cercanas a revolución\n",
      "\n",
      " 1. organización\n",
      " 2. industria\n",
      " 3. jerarquía\n",
      " 4. legalidad\n",
      " 5. unidad\n",
      " 6. cámara\n",
      " 7. nación\n",
      " 8. ilusión\n",
      " 9. recuperación\n",
      "10. brigada\n",
      "================================================================================\n",
      "Palabras más cercanas a francisco\n",
      "\n",
      " 1. jesús\n",
      " 2. lorenzo\n",
      " 3. maría\n",
      " 4. belgrano\n",
      " 5. dolores\n",
      " 6. tercero\n",
      " 7. jardín\n",
      " 8. pablo\n",
      " 9. carlos\n",
      "10. santa\n",
      "================================================================================\n",
      "Palabras más cercanas a maradona\n",
      "\n",
      " 1. allegados\n",
      " 2. vice\n",
      " 3. luciano\n",
      " 4. videla\n",
      " 5. fiel\n",
      " 6. narvaja\n",
      " 7. colegas\n",
      " 8. ama\n",
      " 9. massa\n",
      "10. duhalde\n",
      "================================================================================\n",
      "Palabras más cercanas a matemática\n",
      "\n",
      " 1. calidad\n",
      " 2. tecnología\n",
      " 3. libertad\n",
      " 4. violencia\n",
      " 5. ropa\n",
      " 6. empleos\n",
      " 7. medicina\n",
      " 8. corrupción\n",
      " 9. participación\n",
      "10. escasa\n",
      "================================================================================\n",
      "Palabras más cercanas a ciencia\n",
      "\n",
      " 1. tecnología\n",
      " 2. social\n",
      " 3. izquierda\n",
      " 4. transparencia\n",
      " 5. cultura\n",
      " 6. sa\n",
      " 7. calidad\n",
      " 8. física\n",
      " 9. restauración\n",
      "10. gravedad\n",
      "================================================================================\n",
      "Palabras más cercanas a piqueteros\n",
      "\n",
      " 1. mismos\n",
      " 2. mineros\n",
      " 3. gremios\n",
      " 4. uniformados\n",
      " 5. sindicatos\n",
      " 6. agropecuarios\n",
      " 7. jueces\n",
      " 8. concursos\n",
      " 9. chicos\n",
      "10. cordobeses\n",
      "================================================================================\n",
      "Palabras más cercanas a inseguridad\n",
      "\n",
      " 1. empresa\n",
      " 2. sociedad\n",
      " 3. colectiva\n",
      " 4. demanda\n",
      " 5. sentencia\n",
      " 6. iniciativa\n",
      " 7. norma\n",
      " 8. policía\n",
      " 9. protesta\n",
      "10. ley\n",
      "================================================================================\n",
      "Palabras más cercanas a fútbol\n",
      "\n",
      " 1. contenido\n",
      " 2. ingreso\n",
      " 3. servicio\n",
      " 4. estado\n",
      " 5. régimen\n",
      " 6. financiamiento\n",
      " 7. tratamiento\n",
      " 8. pago\n",
      " 9. uso\n",
      "10. fondo\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'messiargentina'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-620dca148759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m              \u001b[0;34m\"piqueteros\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inseguridad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fútbol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"messi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m              \"argentina\", \"uruguay\", \"china\", \"rusia\", \"eeuu\"]:\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbest_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Palabras más cercanas a {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-620dca148759>\u001b[0m in \u001b[0;36mclosest_words\u001b[0;34m(palabra, num_words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclosest_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalabra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalabra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0morig_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2Ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpalabra\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morig_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'messiargentina'"
     ]
    }
   ],
   "source": [
    "def closest_words(palabra, num_words=20):\n",
    "    if isinstance(palabra, str):\n",
    "        orig_idx = word2Ind[palabra]\n",
    "        vec = M_norm[orig_idx]\n",
    "    else:\n",
    "        vec = palabra\n",
    "\n",
    "    cos_sim = (M_norm @ vec)\n",
    "\n",
    "    best_indices = np.argsort(-cos_sim)[1:num_words+1]\n",
    "    \n",
    "    return [relevant_words[idx] for idx in best_indices]\n",
    "    \n",
    "\n",
    "for word in [\"macri\", \"cristina\", \"izquierda\", \"derecha\", \"revolución\",\n",
    "             \"francisco\", \"maradona\", \"matemática\", \"ciencia\",\n",
    "             \"piqueteros\", \"inseguridad\", \"fútbol\", \"messi\"\n",
    "             \"argentina\", \"uruguay\", \"china\", \"rusia\", \"eeuu\"]:\n",
    "    best_words = closest_words(word, num_words=10)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Palabras más cercanas a {}\\n\".format(word))\n",
    "    for i, close_word in enumerate(best_words):\n",
    "        print(\"{:>2}. {}\".format(i+1, close_word))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda de analogías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profesionalización',\n",
       " 'definidas',\n",
       " 'rendiciones',\n",
       " 'autorizaciones',\n",
       " 'lentes',\n",
       " 'cortinas',\n",
       " 'suspendidas',\n",
       " 'atenderlas',\n",
       " 'exento',\n",
       " 'experiencias',\n",
       " 'renegó',\n",
       " 'garantías',\n",
       " 'convocatorias',\n",
       " 'cubrirse',\n",
       " 'cocheras',\n",
       " 'lecciones',\n",
       " 'señalización',\n",
       " 'angeli',\n",
       " 'asociaciones',\n",
       " 'afecciones']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = M_norm[word2Ind[\"fútbol\"]]\n",
    "y = M_norm[word2Ind[\"messi\"]]\n",
    "z = M_norm[word2Ind[\"finanzas\"]]\n",
    "\n",
    "closest_words(z+y-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Analogies with Word Vectors\n",
    "Word2Vec vectors have been shown to *sometimes* exhibit the ability to solve analogies. \n",
    "\n",
    "As an example, for the analogy \"man : king :: woman : x\", what is x?\n",
    "\n",
    "In the cell below, we show you how to use word vectors to find x. The `most_similar` function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will be the word ranked most similar (largest numerical value).\n",
    "\n",
    "**Note:** Further Documentation on the `most_similar` function can be found within the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to answer the analogy -- man : king :: woman : x\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4: Finding Analogies [code + written]  (2 Points)\n",
    "Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
    "\n",
    "**Note**: You may have to try many analogies to find one that works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your analogy exploration code here.\n",
    "\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=[], negative=[]))\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5: Incorrect Analogy [code + written] (1 point)\n",
    "Find an example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your incorrect analogy exploration code here.\n",
    "\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=[], negative=[]))\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.6: Guided Analysis of Bias in Word Vectors [written] (1 point)\n",
    "\n",
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit to our word embeddings.\n",
    "\n",
    "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"boss\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"boss\" and most dissimilar to \"woman\". What do you find in the top 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'boss'], negative=['man']))\n",
    "print()\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'boss'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.7: Independent Analysis of Bias in Word Vectors [code + written]  (2 points)\n",
    "\n",
    "Use the `most_similar` function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your bias exploration code here.\n",
    "\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=[], negative=[]))\n",
    "print()\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=[,], negative=[]))\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.8: Thinking About Bias [written] (1 point)\n",
    "\n",
    "What might be the cause of these biases in the word vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your SUNET ID above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
    "7. Submit your PDF on Gradescope."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
